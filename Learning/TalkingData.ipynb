{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This kernel have improvement from Pranav Pandya and Andy Harless\n",
    "# Pranav Kernel: https://www.kaggle.com/pranav84/xgboost-on-hist-mode-ip-addresses-dropped\n",
    "# Andy Kernel: https://www.kaggle.com/aharless/jo-o-s-xgboost-with-memory-usage-enhancements\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this for validation with 10% from train\n",
    "is_valid = False\n",
    "\n",
    "path = './input/train.csv'\n",
    "\n",
    "train_df = pd.read_csv(path)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.columns.values\n",
      " ['ip' 'app' 'device' 'os' 'channel' 'click_time' 'attributed_time'\n",
      " 'is_attributed']\n",
      "(100000, 8)\n",
      "ip_count\n",
      "             ip  channel\n",
      "0            0        2\n",
      "1            1       50\n",
      "2            2        6\n",
      "3            3      291\n",
      "4            4       39\n",
      "5            5       99\n",
      "6            6      798\n",
      "7            7        1\n",
      "8            8        2\n",
      "9            9     1385\n",
      "10          10      598\n",
      "11          11        9\n",
      "12          12       30\n",
      "13          13       27\n",
      "14          14        4\n",
      "15          15        1\n",
      "16          16       13\n",
      "17          17        1\n",
      "18          18       18\n",
      "19          19      415\n",
      "20          20     1827\n",
      "21          21       38\n",
      "22          22       15\n",
      "23          23        4\n",
      "24          24        4\n",
      "25          25      608\n",
      "26          26       21\n",
      "27          27      910\n",
      "28          28        3\n",
      "29          29       93\n",
      "...        ...      ...\n",
      "126384  126384       37\n",
      "126385  126385        1\n",
      "126386  126386     1214\n",
      "126387  126387        2\n",
      "126388  126388      804\n",
      "126389  126389      347\n",
      "126390  126390       24\n",
      "126391  126391        2\n",
      "126392  126392      470\n",
      "126393  126393        2\n",
      "126394  126394       40\n",
      "126395  126395       11\n",
      "126396  126396        5\n",
      "126397  126397       45\n",
      "126398  126398        1\n",
      "126399  126399       57\n",
      "126400  126400      634\n",
      "126401  126401     1093\n",
      "126402  126402        2\n",
      "126403  126403      126\n",
      "126404  126404        7\n",
      "126405  126405       21\n",
      "126406  126406       29\n",
      "126407  126407        1\n",
      "126408  126408      642\n",
      "126409  126409        4\n",
      "126410  126410     1032\n",
      "126411  126411      466\n",
      "126412  126412       34\n",
      "126413  126413        2\n",
      "\n",
      "[126414 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# This kernel have improvement from Pranav Pandya and Andy Harless\n",
    "# Pranav Kernel: https://www.kaggle.com/pranav84/xgboost-on-hist-mode-ip-addresses-dropped\n",
    "# Andy Kernel: https://www.kaggle.com/aharless/jo-o-s-xgboost-with-memory-usage-enhancements\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this for validation with 10% from train\n",
    "is_valid = False\n",
    "\n",
    "path = 'train_sample.csv'\n",
    "\n",
    "def timeFeatures(df):\n",
    "    # Make some new features with click_time column\n",
    "    df['datetime'] = pd.to_datetime(df['click_time'])\n",
    "    df['dow']      = df['datetime'].dt.dayofweek\n",
    "    df[\"doy\"]      = df[\"datetime\"].dt.dayofyear\n",
    "    #df[\"dteom\"]    = df[\"datetime\"].dt.daysinmonth - df[\"datetime\"].dt.day\n",
    "    df.drop(['click_time', 'datetime'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "# Read the last lines because they are more impacting in training than the starting lines\n",
    "train_df = pd.read_csv(path)\n",
    "print('train_df.columns.values\\n', train_df.columns.values)\n",
    "print(train_df.shape)\n",
    "\n",
    "ip_count = merge.groupby(['ip'])['channel'].count().reset_index()\n",
    "print('ip_count\\n', ip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.columns.values\n",
      " ['ip' 'app' 'device' 'os' 'channel' 'click_time' 'attributed_time'\n",
      " 'is_attributed']\n",
      "[57.53672504425049] Finished to load data\n",
      "[91.68102288246155] Start to generate time features\n",
      "[91.93506002426147] Start XGBoost Training\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[19:30:28] src/objective/regression_obj.cc:43: Check failed: info.labels.size() != 0U (0 vs. 0) label set cannot be empty\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x0000001a17299aa8 _ZN4dmlc15LogMessageFatalD2Ev + 40\\n[bt] (1) 1   libxgboost.dylib                    0x0000001a17302ff9 _ZN7xgboost3obj10RegLossObjINS0_22LogisticClassificationEE11GetGradientERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEiPNS5_INS_6detail18bst_gpair_internalIfEENS6_ISG_EEEE + 281\\n[bt] (2) 2   libxgboost.dylib                    0x0000001a172960f6 _ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE + 1014\\n[bt] (3) 3   libxgboost.dylib                    0x0000001a172af73f XGBoosterUpdateOneIter + 79\\n[bt] (4) 4   libffi.6.dylib                      0x0000000111130884 ffi_call_unix64 + 76\\n[bt] (5) 5   ???                                 0x00007ffee05bd330 0x0 + 140732662534960\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-21f1228a37b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[19:30:28] src/objective/regression_obj.cc:43: Check failed: info.labels.size() != 0U (0 vs. 0) label set cannot be empty\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x0000001a17299aa8 _ZN4dmlc15LogMessageFatalD2Ev + 40\\n[bt] (1) 1   libxgboost.dylib                    0x0000001a17302ff9 _ZN7xgboost3obj10RegLossObjINS0_22LogisticClassificationEE11GetGradientERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEiPNS5_INS_6detail18bst_gpair_internalIfEENS6_ISG_EEEE + 281\\n[bt] (2) 2   libxgboost.dylib                    0x0000001a172960f6 _ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE + 1014\\n[bt] (3) 3   libxgboost.dylib                    0x0000001a172af73f XGBoosterUpdateOneIter + 79\\n[bt] (4) 4   libffi.6.dylib                      0x0000000111130884 ffi_call_unix64 + 76\\n[bt] (5) 5   ???                                 0x00007ffee05bd330 0x0 + 140732662534960\\n'"
     ]
    }
   ],
   "source": [
    "# This kernel have improvement from Pranav Pandya and Andy Harless\n",
    "# Pranav Kernel: https://www.kaggle.com/pranav84/xgboost-on-hist-mode-ip-addresses-dropped\n",
    "# Andy Kernel: https://www.kaggle.com/aharless/jo-o-s-xgboost-with-memory-usage-enhancements\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this for validation with 10% from train\n",
    "is_valid = False\n",
    "\n",
    "path_train = './input/train_sample.csv'\n",
    "path_test = './input/test_supplement.csv'\n",
    "\n",
    "def timeFeatures(df):\n",
    "    # Make some new features with click_time column\n",
    "    df['datetime'] = pd.to_datetime(df['click_time'])\n",
    "    df['dow']      = df['datetime'].dt.dayofweek\n",
    "    df[\"doy\"]      = df[\"datetime\"].dt.dayofyear\n",
    "    #df[\"dteom\"]    = df[\"datetime\"].dt.daysinmonth - df[\"datetime\"].dt.day\n",
    "    df.drop(['click_time', 'datetime'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "# Read the last lines because they are more impacting in training than the starting lines\n",
    "train_df = pd.read_csv(path_train)\n",
    "print('train_df.columns.values\\n',train_df.columns.values)\n",
    "# params: usecols-какие столбцы в данных будем использовать, skiprows-пропуст строк в диапозоне, nrows-чтение кол-во строк,   \n",
    "train_df = pd.read_csv(path_train, skiprows=range(1,123903891), nrows=61000000, usecols=train_columns, dtype=dtypes)\n",
    "test_df = pd.read_csv(path_test, usecols=test_columns, dtype=dtypes)\n",
    "\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "\n",
    "# Drop the IP and the columns from target\n",
    "y = train_df['is_attributed']\n",
    "#  params: axis-whether to drop labels from the index (0/‘index’) or columns (1/‘columns’),\n",
    "# inplace-If True, do operation inplace and return None.\n",
    "train_df.drop(['is_attributed'], axis=1, inplace=True)\n",
    "\n",
    "# Drop IP and ID from test rows\n",
    "sub = pd.DataFrame()\n",
    "#sub['click_id'] = test['click_id'].astype('int')\n",
    "test_df.drop(['click_id'], axis=1, inplace=True)\n",
    "# сборщик мусора\n",
    "gc.collect()\n",
    "# кол-во строк\n",
    "nrow_train = train_df.shape[0]\n",
    "# объединяем две выборки\n",
    "merge = pd.concat([train_df, test_df])\n",
    "\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "# Count the number of clicks by ip\n",
    "ip_count = merge.groupby(['ip'])['channel'].count().reset_index()\n",
    "# print('ip_count', ip_count)\n",
    "ip_count.columns = ['ip', 'clicks_by_ip']\n",
    "# params: merge-выборка, включающая train_df, test_df. on-имена полей для присоединения, \n",
    "# how-использует только ключи из левого кадра, похожие на внешнее соединение SQL слева\n",
    "merge = pd.merge(merge, ip_count, on='ip', how='left', sort=False)\n",
    "merge['clicks_by_ip'] = merge['clicks_by_ip'].astype('uint16')\n",
    "merge.drop('ip', axis=1, inplace=True)\n",
    "\n",
    "train_df = merge[:nrow_train]\n",
    "test_df = merge[nrow_train:]\n",
    "\n",
    "del test_df, merge\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Start to generate time features'.format(time.time() - start_time))\n",
    "\n",
    "train_df = timeFeatures(train_df)\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Start XGBoost Training'.format(time.time() - start_time))\n",
    "\n",
    "# Set the params(this params from Pranav kernel) for xgboost model\n",
    "params = {'eta': 0.3,\n",
    "          'tree_method': \"hist\",\n",
    "          'grow_policy': \"lossguide\",\n",
    "          'max_leaves': 1400,  \n",
    "          'max_depth': 0, \n",
    "          'subsample': 0.9, \n",
    "          'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':0,\n",
    "          'alpha':4,\n",
    "          'objective': 'binary:logistic', \n",
    "          'scale_pos_weight':9,\n",
    "          'eval_metric': 'auc', \n",
    "          'nthread':8,\n",
    "          'random_state': 99, \n",
    "          'silent': True}\n",
    "          \n",
    "\n",
    "if (is_valid == True):\n",
    "    # Get 10% of train dataset to use as validation\n",
    "    x1, x2, y1, y2 = train_test_split(train_df, y, test_size=0.1, random_state=99)\n",
    "    dtrain = xgb.DMatrix(x1, y1)\n",
    "    dvalid = xgb.DMatrix(x2, y2)\n",
    "    del x1, y1, x2, y2 \n",
    "    gc.collect()\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "    model = xgb.train(params, dtrain, 200, watchlist, maximize=True, early_stopping_rounds = 25, verbose_eval=5)\n",
    "    del dvalid\n",
    "else:\n",
    "    dtrain = xgb.DMatrix(train_df, y)\n",
    "    del train_df, y\n",
    "    gc.collect()\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, 30, watchlist, maximize=True, verbose_eval=1)\n",
    "\n",
    "del dtrain\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Finish XGBoost Training'.format(time.time() - start_time))\n",
    "\n",
    "# Plot the feature importance from xgboost\n",
    "plot_importance(model)\n",
    "plt.gcf().savefig('feature_importance_xgb.png')\n",
    "\n",
    "# Load the test for predict \n",
    "test = pd.read_csv(\"test.csv\", usecols=test_columns, dtype=dtypes)\n",
    "test = pd.merge(test, ip_count, on='ip', how='left', sort=False)\n",
    "del ip_count\n",
    "gc.collect()\n",
    "\n",
    "sub['click_id'] = test['click_id'].astype('int')\n",
    "\n",
    "test['clicks_by_ip'] = test['clicks_by_ip'].astype('uint16')\n",
    "test = timeFeatures(test)\n",
    "test.drop(['click_id', 'ip'], axis=1, inplace=True)\n",
    "dtest = xgb.DMatrix(test)\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "# Save the predictions\n",
    "sub['is_attributed'] = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "sub.to_csv('xgb_sub.csv', float_format='%.8f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64.60566782951355] Finished to load data\n",
      "[105.4098949432373] Start to generate time features\n",
      "[105.69547700881958] Start XGBoost Training\n",
      "train\n",
      " Empty DataFrame\n",
      "Columns: [app, device, os, channel, clicks_by_ip, dow, doy]\n",
      "Index: []\n",
      "dtrain\n",
      " <xgboost.core.DMatrix object at 0x1a180d2cc0>\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[21:08:16] src/objective/regression_obj.cc:43: Check failed: info.labels.size() != 0U (0 vs. 0) label set cannot be empty\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x0000001a17299aa8 _ZN4dmlc15LogMessageFatalD2Ev + 40\\n[bt] (1) 1   libxgboost.dylib                    0x0000001a17302ff9 _ZN7xgboost3obj10RegLossObjINS0_22LogisticClassificationEE11GetGradientERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEiPNS5_INS_6detail18bst_gpair_internalIfEENS6_ISG_EEEE + 281\\n[bt] (2) 2   libxgboost.dylib                    0x0000001a172960f6 _ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE + 1014\\n[bt] (3) 3   libxgboost.dylib                    0x0000001a172af73f XGBoosterUpdateOneIter + 79\\n[bt] (4) 4   libffi.6.dylib                      0x0000000111130884 ffi_call_unix64 + 76\\n[bt] (5) 5   ???                                 0x00007ffee05bd330 0x0 + 140732662534960\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7f7fd2301ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m#     print('train_before_xgb\\n', dtrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[21:08:16] src/objective/regression_obj.cc:43: Check failed: info.labels.size() != 0U (0 vs. 0) label set cannot be empty\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x0000001a17299aa8 _ZN4dmlc15LogMessageFatalD2Ev + 40\\n[bt] (1) 1   libxgboost.dylib                    0x0000001a17302ff9 _ZN7xgboost3obj10RegLossObjINS0_22LogisticClassificationEE11GetGradientERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEiPNS5_INS_6detail18bst_gpair_internalIfEENS6_ISG_EEEE + 281\\n[bt] (2) 2   libxgboost.dylib                    0x0000001a172960f6 _ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE + 1014\\n[bt] (3) 3   libxgboost.dylib                    0x0000001a172af73f XGBoosterUpdateOneIter + 79\\n[bt] (4) 4   libffi.6.dylib                      0x0000000111130884 ffi_call_unix64 + 76\\n[bt] (5) 5   ???                                 0x00007ffee05bd330 0x0 + 140732662534960\\n'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this for validation with 10% from train\n",
    "is_valid = False\n",
    "\n",
    "path = './input/'\n",
    "\n",
    "def timeFeatures(df):\n",
    "    # Make some new features with click_time column\n",
    "    df['datetime'] = pd.to_datetime(df['click_time'])\n",
    "    df['dow']      = df['datetime'].dt.dayofweek\n",
    "    df[\"doy\"]      = df[\"datetime\"].dt.dayofyear\n",
    "    #df[\"dteom\"]    = df[\"datetime\"].dt.daysinmonth - df[\"datetime\"].dt.day\n",
    "    df.drop(['click_time', 'datetime'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "# Read the last lines because they are more impacting in training than the starting lines\n",
    "train = pd.read_csv(path+\"train.csv\", skiprows=range(1,123903891), nrows=61000000, usecols=train_columns, dtype=dtypes)\n",
    "test = pd.read_csv(path+\"test_supplement.csv\", usecols=test_columns, dtype=dtypes)\n",
    "\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "\n",
    "# Drop the IP and the columns from target\n",
    "y = train['is_attributed']\n",
    "train.drop(['is_attributed'], axis=1, inplace=True)\n",
    "\n",
    "# Drop IP and ID from test rows\n",
    "sub = pd.DataFrame()\n",
    "#sub['click_id'] = test['click_id'].astype('int')\n",
    "test.drop(['click_id'], axis=1, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "nrow_train = train.shape[0]\n",
    "merge = pd.concat([train, test])\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "# Count the number of clicks by ip\n",
    "ip_count = merge.groupby(['ip'])['channel'].count().reset_index()\n",
    "ip_count.columns = ['ip', 'clicks_by_ip']\n",
    "merge = pd.merge(merge, ip_count, on='ip', how='left', sort=False)\n",
    "merge['clicks_by_ip'] = merge['clicks_by_ip'].astype('uint16')\n",
    "merge.drop('ip', axis=1, inplace=True)\n",
    "\n",
    "train = merge[:nrow_train]\n",
    "test = merge[nrow_train:]\n",
    "\n",
    "del test, merge\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Start to generate time features'.format(time.time() - start_time))\n",
    "\n",
    "train = timeFeatures(train)\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Start XGBoost Training'.format(time.time() - start_time))\n",
    "\n",
    "# Set the params(this params from Pranav kernel) for xgboost model\n",
    "# params = {'eta': 0.3,\n",
    "#           'tree_method': \"hist\",\n",
    "#           'grow_policy': \"lossguide\",\n",
    "#           'max_leaves': 1400,  \n",
    "#           'max_depth': 0, \n",
    "#           'subsample': 0.9, \n",
    "#           'colsample_bytree': 0.7, \n",
    "#           'colsample_bylevel':0.7,\n",
    "#           'min_child_weight':0,\n",
    "#           'alpha':4,\n",
    "#           'objective': 'binary:logistic', \n",
    "#           'scale_pos_weight':9,\n",
    "#           'eval_metric': 'auc', \n",
    "#           'nthread':8,\n",
    "#           'random_state': 99, \n",
    "#           'silent': True}\n",
    " \n",
    "params = {'eta': 0.3,\n",
    "          'tree_method': \"hist\",\n",
    "          'grow_policy': \"lossguide\",\n",
    "          'max_leaves': 1400,  \n",
    "          'max_depth': 0, \n",
    "          'subsample': 0.9, \n",
    "          'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':0,\n",
    "          'alpha':4,\n",
    "#           'mlogloss': 'multi:softprob',\n",
    "          'objective': 'binary:logistic', \n",
    "          'scale_pos_weight':9,\n",
    "          'eval_metric': 'auc', \n",
    "          'nthread':8,\n",
    "          'random_state': 99, \n",
    "          'silent': True}\n",
    " \n",
    "\n",
    "if (is_valid == True):\n",
    "    # Get 10% of train dataset to use as validation\n",
    "    x1, x2, y1, y2 = train_test_split(train, y, test_size=0.1, random_state=99)\n",
    "    dtrain = xgb.DMatrix(x1, y1)\n",
    "    dvalid = xgb.DMatrix(x2, y2)\n",
    "    del x1, y1, x2, y2 \n",
    "    gc.collect()\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "    model = xgb.train(params, dtrain, 200, watchlist, maximize=True, early_stopping_rounds = 25, verbose_eval=5)\n",
    "    del dvalid\n",
    "else:\n",
    "    print('train\\n', train.head(10))\n",
    "    dtrain = xgb.DMatrix(train, y)\n",
    "    print('dtrain\\n', dtrain)\n",
    "    \n",
    "    del train, y\n",
    "    gc.collect()\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "#     print('train_before_xgb\\n', dtrain)\n",
    "    model = xgb.train(params, dtrain, 30, watchlist, maximize=True, verbose_eval=1)\n",
    "\n",
    "del dtrain\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Finish XGBoost Training'.format(time.time() - start_time))\n",
    "\n",
    "# Plot the feature importance from xgboost\n",
    "plot_importance(model)\n",
    "plt.gcf().savefig('feature_importance_xgb.png')\n",
    "\n",
    "# Load the test for predict \n",
    "test = pd.read_csv(path+\"test.csv\", usecols=test_columns, dtype=dtypes)\n",
    "test = pd.merge(test, ip_count, on='ip', how='left', sort=False)\n",
    "del ip_count\n",
    "gc.collect()\n",
    "\n",
    "sub['click_id'] = test['click_id'].astype('int')\n",
    "\n",
    "test['clicks_by_ip'] = test['clicks_by_ip'].astype('uint16')\n",
    "test = timeFeatures(test)\n",
    "test.drop(['click_id', 'ip'], axis=1, inplace=True)\n",
    "dtest = xgb.DMatrix(test)\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "# Save the predictions\n",
    "sub['is_attributed'] = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "sub.to_csv('xgb_sub.csv', float_format='%.8f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
